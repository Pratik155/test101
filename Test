from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
import numpy as np

# Define your list of stop words
stop_words = set(["your", "list", "of", "stop", "words", "here"])

# Assuming df_sentences contains a column 'sentences' and df_categories contains a column 'categories'
sentences = df_sentences['sentences']
categories = df_categories['categories']

# Convert categories to lowercase
categories = categories.apply(lambda x: [cat.lower() for cat in x])

# Flatten the list of categories
categories_flat = [cat for sublist in categories for cat in sublist]

# Create TF-IDF vectorizer with stop words removal
tfidf_vectorizer = TfidfVectorizer(ngram_range=(1, 2), stop_words=stop_words)

# Fit and transform sentences to TF-IDF vectors
tfidf_sentences = tfidf_vectorizer.fit_transform(sentences)

# Initialize a dictionary to store sentence-category mappings
sentence_category_mapping = {}

# Iterate over each sentence
for i, sentence in enumerate(sentences):
    # Calculate cosine similarity between sentence and each category
    similarities = cosine_similarity(tfidf_sentences[i], tfidf_vectorizer.transform(categories_flat))
    # Find the index of the category with highest similarity
    max_sim_idx = np.argmax(similarities)
    # Map the sentence to the corresponding category
    sentence_category_mapping[sentence] = categories_flat[max_sim_idx]

# Print the mapping
for sentence, category in sentence_category_mapping.items():
    print(f"Sentence: {sentence} -> Category: {category}")
